{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9645be8",
   "metadata": {},
   "source": [
    "# Sam Jackson, MAE 494, HW4\n",
    "\n",
    "This code provides answers to the questions posed in Homework 4. The packages used in this homework assignment are numpy, and scipy.optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e07e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8700c353",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "To solve the problem graphically, as well as show the feasible descent directions and directions of $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial g}{\\partial x}s $ at the corner points, I have drawn out the problem on a physical notebook and uploaded it in the following 2 images below...\n",
    "\n",
    "![MAE494HW4Pa](img/MAE494HW4Pa.jpg)\n",
    "\n",
    "![MAE494HW4Pb](img/MAE494HW4Pb.jpg)\n",
    "\n",
    "To verify this graphical result the KKT conditions were used, as shown below...\n",
    "\n",
    "First the Lagrangian function, $L(x,\\lambda,\\mu)$, was created for our specific case.\n",
    "\n",
    "$L = (x_1 +1)^2+(x_2-2)^2+\\mu_1(x_1-2)+\\mu_2(x_2-1)+\\mu_3(-x_1)+\\mu_4(-x_2)$\n",
    "\n",
    "Then taking the derivative of this function with respect to $x$.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}=\\left[ \\begin{array}{c} \\frac{\\partial L}{\\partial x_1} \\\\ \\frac{\\partial L}{\\partial x_2} \\end{array} \\right]=\\left[ \\begin{array}{c} 2(x_1+1)+\\mu_1-\\mu_3 \\\\ 2(x_2-2)+\\mu_2-\\mu_4 \\end{array} \\right]$ \n",
    "\n",
    "Then plugging in information gathered from the graphical solution to verify that it is correct. Thus, we expect that since $g_2$ and $g_3$ are the only active constraints at the found solution of $x=[0,1]^T$, $\\mu_2 \\gt 0$ and $\\mu_3 \\gt 0$ while $\\mu_1=\\mu_4=0$. Verifying this by checking this specific case...\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}=\\left[ \\begin{array}{c} 2(x_1+1)+\\mu_1-\\mu_3 \\\\ 2(x_2-2)+\\mu_2-\\mu_4 \\end{array} \\right]=\\left[ \\begin{array}{c} 2((0)+1)+(0)-\\mu_3 \\\\ 2((1)-2)+\\mu_2-(0) \\end{array} \\right]=\\left[ \\begin{array}{c} 2-\\mu_3 \\\\ -2+\\mu_2 \\end{array} \\right]=\\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]$ \n",
    "\n",
    "Thus we can see that our expectations are confirmed that this is the constrained optimum case, as... \n",
    "\n",
    "$\\left[ \\begin{array}{c} \\mu_3 \\\\ \\mu_2 \\end{array} \\right]=\\left[ \\begin{array}{c} 2 \\\\ 2 \\end{array} \\right] \\gt \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]$ \n",
    "\n",
    "We can also verify that this problem is strictly convex by taking the Hessian of the Lagrangian function and proving it is positive definite, as shown below...\n",
    "\n",
    "$\\frac{\\partial^2 L}{\\partial x^2}=\\left[ \\begin{array}{cc} \\frac{\\partial^2 L}{\\partial x_1^2} & \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2}\\\\ \\frac{\\partial^2 L}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 L}{\\partial x_2^2} \\end{array} \\right]=\\left[ \\begin{array}{cc} 2 & 0\\\\ 0 & 2\\end{array} \\right] \\Rightarrow \\lambda = 2 > 0 \\therefore \\frac{\\partial^2 L}{\\partial x^2}$ is positive definite. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd66aed5",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "To start this problem, I first adjusted $g_2$ so that it followed the standard form of $g\\le 0$, thus $g_2 = -x_2 \\le 0$. Then I set about graphing the problem and finding the solution graphically, as shown below...\n",
    "\n",
    "![MAE494HW4Pc](img/MAE494HW4Pc.jpg)\n",
    "\n",
    "As we can see, the solution found graphically was $x=[1,0]^T$. To verify this solution, the KKT optimality conditions were applied to the problem, as shown below...\n",
    "\n",
    "First creating the Lagrangian function with only inequality constraints.\n",
    "\n",
    "$L=f+\\mu^T g=-x_1+\\mu_1(x_2-(1-x_1)^3+\\mu_2(-x_2)$\n",
    "\n",
    "Then taking the gradient w.r.t. x, plugging in $x=[1,0]^T$ and setting it to equal to 0 to confirm it is a stationary point.\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}=\\left[ \\begin{array}{c} -1+\\mu_1(-3(1-x_1)^2(-1)) \\\\ \\mu_1-\\mu_2 \\end{array} \\right] = \\left[ \\begin{array}{c} -1+\\mu_1(-3(1-(1))^2(-1)) \\\\ \\mu_1-\\mu_2 \\end{array} \\right] = \\left[ \\begin{array}{c} -1 \\\\ \\mu_1-\\mu_2 \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ 0 \\end{array} \\right]$ \n",
    "\n",
    "As we can see, the requirement for the gradient to be equal to 0 is not fulfilled, and even cannot be fulfilled for any valid point using these KTT conditions. This is because the constrained optimum found graphically is in fact an irregular point. This can be proven by showing that there is degeneracy, as $\\frac{\\partial g_1}{\\partial x}$ and $\\frac{\\partial g_2}{\\partial x}$ are linearly dependant at the optimum, shown below...\n",
    "\n",
    "$\\frac{\\partial g_1}{\\partial x} = \\left[ \\begin{array}{c} -3(1-x_1)^2(-1) \\\\ 1 \\end{array} \\right]= \\left[ \\begin{array}{c} 3(1-(1))^2 \\\\ 1 \\end{array} \\right]=\\left[ \\begin{array}{c} 0 \\\\ 1 \\end{array} \\right]$\n",
    "\n",
    "$\\frac{\\partial g_1}{\\partial x} = \\left[ \\begin{array}{c} 0 \\\\ -1 \\end{array} \\right]$\n",
    "\n",
    "$\\frac{\\partial g_1}{\\partial x}=-\\frac{\\partial g_2}{\\partial x}$\n",
    "\n",
    "So in short, while the solution was able to be found graphically, the KKT optimality conditions were unable to find the constrained optimum of $x=[1,0]^T$ because this is an irregular point as the gradients of the two inequality constraints are linearly dependant at that point.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3a41d",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "The local solution to this problem was found using both the reduced gradient and Lagrange multiplier methods.\n",
    "\n",
    "$$\\begin{aligned} &\\text{max}_{x_1,x_2,x_3} && x_1x_2+x_2x_3+x_1x_3 \\\\ &\\text{subject to} && h=x_1+x_2+x_3-3=0 \\end{aligned}$$\n",
    "\n",
    "#### Reduced Gradient Method\n",
    "\n",
    "$\\frac{df}{dd}=\\frac{\\partial f}{\\partial d}-\\frac{\\partial f}{\\partial s}(\\frac{\\partial h}{\\partial s})^{-1} \\frac{\\partial h}{\\partial d}$\n",
    "\n",
    "First, selecting state and decision variables. As there is only one equality constraint there only needs to be one state variable, chosen to be $s=[x_1]$. Thus, the decision variables are $d=[x_2,x_3]$.\n",
    "\n",
    "Next, finding $\\frac{\\partial f}{\\partial s},\\frac{\\partial f}{\\partial d},\\frac{\\partial h}{\\partial s},$ and $\\frac{\\partial f}{\\partial d}$ to plug into the reduced gradient formula...\n",
    "\n",
    "$\\frac{\\partial f}{\\partial s}=[x_2+x_3]$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial d}=[x_1+x_3,x_1+x_2]$\n",
    "\n",
    "$\\frac{\\partial h}{\\partial s}=[1]$\n",
    "\n",
    "$\\frac{\\partial h}{\\partial d}=[1,1]$\n",
    "\n",
    "Finally, plugging all of these into the reduced gradient formula and using these equations with h=0 to solve for the points where the reduced gradient is equal to zero...\n",
    "\n",
    "$[0,0]=\\frac{\\partial f}{\\partial d}-\\frac{\\partial f}{\\partial s}(\\frac{\\partial h}{\\partial s})^{-1} \\frac{\\partial h}{\\partial d}= [x_1+x_3,x_1+x_2]-[x_2+x_3][1]^{-1}[1,1]=[(x_1+x_3)-(x_2+x_3),(x_1+x_2)-(x_2+x_3)]=[x_1-x_2,x_1-x_3]$\n",
    "\n",
    "Thus our three equations are...\n",
    "\n",
    "$x_1+x_2+x_3-3=0$\n",
    "\n",
    "$x_1-x_2=0$\n",
    "\n",
    "$x_1-x_3=0$\n",
    "\n",
    "Rearranging to solve for x...\n",
    "\n",
    "$x_1=x_2$ & $x_1=x_3 \\Rightarrow x_1=x_2=x_3$ \n",
    "\n",
    "$x_1+x_2+x_3-3=0 \\Rightarrow 3(x_1)=3 \\Rightarrow x_1=1$\n",
    "\n",
    "$x_1=x_2=x_3=1$\n",
    "\n",
    "Thus, these are the values of x at a stationary point of this constrained problem, however at this stage we do not know if this is a minimum or a maximum through the second order sufficiency conditions. This will be determined during the solution using Lagrange multipliers.\n",
    "\n",
    "#### Lagrange Multipliers Method\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}= \\frac{\\partial f}{\\partial x} +\\lambda^T \\frac{\\partial h}{\\partial x}$\n",
    "\n",
    "First, the Lagrange Multipliers method starts out very similar to the reduced gradient method, with deciding state and decision variables. Choosing to keep these consistant, $s=[x_1]$, and $d=[x_2,x_3]$, this way we do not need to recalculate $\\frac{\\partial f}{\\partial s}$ and $\\frac{\\partial h}{\\partial s}$ to find the Lagrange multipliers, as we do below...\n",
    "\n",
    "$\\lambda^T=-\\frac{\\partial f}{\\partial s}(\\frac{\\partial h}{\\partial s})^{-1} = -[x_2+x_3][1]^{-1}$\n",
    "\n",
    "$\\lambda^T=-[x_2+x_3]$\n",
    "\n",
    "Next, constructing the Lagrangian function, L, using $\\lambda^T$...\n",
    "\n",
    "$L=f+\\lambda^T h$\n",
    "\n",
    "$L=(x_1x_2+x_2x_3+x_1x_3)+[-x_2-x_3](x_1+x_2+x_3-3)$\n",
    "\n",
    "Next, finding the gradient of the Lagrangian w.r.t. each of the x's and setting these gradients equal to 0...\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x}=\\left[ \\begin{array}{c} (x_2+x_3)+(-x_2-x_3)(1)+(0)(x_1+x_2+x_3-3) \\\\ (x_1+x_3)+(-x_2-x_3)(1)+(-1)(x_1+x_2+x_3-3) \\\\ (x_1+x_2)+(-x_2-x_3)(1)+(-1)(x_1+x_2+x_3-3) \\end{array} \\right]=\\left[ \\begin{array}{c} (x_2+x_3)-(x_2+x_3) \\\\ (x_1-x_2)-(x_1+x_2+x_3-3) \\\\ (x_1-x_3)-(x_1+x_2+x_3-3) \\end{array} \\right]=\\left[ \\begin{array}{c} 0 \\\\ (-2x_2-x_3+3) \\\\ (-x_2-2x_3+3) \\end{array} \\right]=\\left[ \\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\end{array} \\right]$\n",
    "\n",
    "To find the last equation required, set the gradient of the Lagrangian w.r.t. $\\lambda$ equal to 0 as well (in other words set h=0)... \n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\lambda}=x_1+x_2+x_3-3=0$\n",
    "\n",
    "Now we can put these three equations together to find the stationary point that fulfills the constraint conditions...\n",
    "\n",
    "$-2x_2-x_3+3=0 \\Rightarrow x_3=3-2x_2$\n",
    "\n",
    "$-x_2-2x_3+3=0 \\Rightarrow -x_2-2(3-2x_2)+3=0 \\Rightarrow 3x_2-3=0 \\Rightarrow x_2=1$\n",
    "\n",
    "$-2x_2-x_3+3=0 \\Rightarrow x_3=3-2(1) \\Rightarrow x_3=1$\n",
    "\n",
    "$x_1+x_2+x_3-3=0 \\Rightarrow x_1+2-3=0 \\Rightarrow x_1=1$\n",
    "\n",
    "$\\Rightarrow x_1=x_2=x_3=1$\n",
    "\n",
    "We can see that we got the same result as the reduced gradient method using the Lagrange multiplier method, but now we need to prove that this is infact the maximum of the constrained problem. This can be done by finding the eigenvalues of the Hessian of the Lagrangian function, $\\frac{\\partial^2 L}{\\partial^2 x}$, to see if they $\\le 0$ (as we want the stationary point to be at least a local maximum). First, we find the Hessian of $L$...\n",
    "\n",
    "$\\frac{\\partial^2 L}{\\partial x^2}=\\left[ \\begin{array}{ccc} 0 & 0 & 0 \\\\\\ 0 & -2 & -1 \\\\\\ 0 & -1 & -2 \\end{array} \\right]$\n",
    "\n",
    "Then we find the eigenvalues of the Hessian...\n",
    "\n",
    "$\\left| \\begin{array}{ccc} 0-\\lambda & 0 & 0 \\\\\\ 0 & -2-\\lambda & -1 \\\\\\ 0 & -1 & -2-\\lambda \\end{array} \\right|=0 \\Rightarrow -\\lambda((-2-\\lambda)(-2-\\lambda)-(-1)(-1))=0 \\Rightarrow \\lambda(\\lambda^2+4\\lambda+3)=0 \\Rightarrow \\lambda(\\lambda + 3)(\\lambda +1)=0$\n",
    "\n",
    "$\\lambda=-3, -1, 0$\n",
    "\n",
    "As we can see, the eigenvalues are $\\le 0$ therefore the Hessian is positive semi-definite. This means that the solution that we found is at least a local maximum, therefore fulfilling the second order sufficiency conditions and proving that this is a valid solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ebfbe",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Solving the following constrained optimization problem using the reduced gradient method to find the value of b for which $x=[1,2]^T$ is the solution...\n",
    "\n",
    "$$\\begin{aligned} &\\text{max}_{x_1,x_2} && 2x_1+bx_2 \\\\ &\\text{subject to} && g_1=x_1^2+x_2^2-5 \\le 0 \\\\\\ && g_2=x_1-x_2-2 \\le 0 \\end{aligned}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bfb314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
