{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ca4148",
   "metadata": {},
   "source": [
    "# Sam Jackson, MAE 494, HW2\n",
    "\n",
    "This code provides answers to the questions posed in Homework 2. The packages used in this homework assignment are numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb36a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad402106",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "First, finding the stationary point by finding the gradient of the function and setting it equal to 0...\n",
    "\n",
    "$f(x)= 2x_1^2-4x_1x_2+1.5x_2^2+x_2$\n",
    "\n",
    "$g(x)=\\begin{bmatrix}\\frac{\\partial f(x)}{\\partial x_1}\\\\ \\frac{\\partial f(x)}{\\partial x_2}\\end{bmatrix} = \\begin{bmatrix}4x_1-4x_2\\\\ -4x_1+3x_2+1\\end{bmatrix} = 0$\n",
    "\n",
    "Therefore, $x_1=1 , x_2=1$ is the stationary point.\n",
    "\n",
    "Now showing that this is a saddle point by proving it has an indefinite Hessian...\n",
    "\n",
    "$H(x)=\\begin{bmatrix}\\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2}\\\\ \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2}\\end{bmatrix}=\\begin{bmatrix} 4 & -4 \\\\ -4 & 3\\end{bmatrix}$\n",
    "\n",
    "Finding the eigenvalues of the Hessian...\n",
    "\n",
    "$0=|H(x)-\\lambda I|=\\bigg|\\begin{array} (4-\\lambda & -4 \\\\ -4 & 3-\\lambda \\end{array}\\bigg|=(4-\\lambda)(3-\\lambda)-(-4)(-4)=\\lambda^2-7\\lambda-4$\n",
    "\n",
    "$\\lambda=7.531, -0.531$\n",
    "\n",
    "Confirming this using Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41768039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda= [ 7.53112887 -0.53112887]\n"
     ]
    }
   ],
   "source": [
    "H=np.array([[4,-4],[-4,3]])\n",
    "eigs=np.linalg.eigvals(H)\n",
    "print('Lambda=',eigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba323bfa",
   "metadata": {},
   "source": [
    "Thus, since one of the eigenvalues of the Hessian is positive and the other is negative the Hessian is considered indefinite and the stationary point is a saddle point.\n",
    "\n",
    "# WIP Direction of downslopes and directions that reduce f "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4b5b41",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "##### Formulating the problem\n",
    "\n",
    "First, formulating the problem as a minimization problem. Using the length formula to find the distance from the point at $(-1,0,1)^T$ to the point on the plane $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "$F(x)=\\sqrt{(x_1-(-1))^2+(x_2-0)^2+(x_3-1)^2}$\n",
    "\n",
    "Since we are looking for the nearest point, this equation is what we are trying to minimize while being subject to the equation $x_1+2x_2+3x_3=1$. $F$ contains an overall square root which makes taking the double partial derivative more difficult but does not affect the point in which the minimum occurs at (minimum of $f$ occurs at same point as $f^2$). Because of this, I choose to define a new function to minimize, $f$, where $f=F^2$. This makes analysis such as finding the Hessian of $f$ significantly easier whilst still producing the point which minimizes the length between the plane and the point. Putting this into the familiar minimization form...\n",
    "\n",
    "$$\\begin{aligned} &\\text{minimize:} && (x_1+1)^2+x_2^2+(x_3-1)^2 \\\\ &\\text{subject to:} && x_1+2x_2+3x_3-1=0 \\end{aligned}$$\n",
    "\n",
    "Putting this into Python to solve for the nearest point..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915572a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474abf9",
   "metadata": {},
   "source": [
    "##### Is this a convex problem?\n",
    "\n",
    "Next, answering the question of if this is a convex problem. First, let's find the eigenvalues of the Hessian to find if it is  at least positive semi-definite.\n",
    "\n",
    "$H(x)=\\begin{bmatrix}\\frac{\\partial^2 f(x)}{\\partial x_1^2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_2} & \\frac{\\partial^2 f(x)}{\\partial x_1 \\partial x_3}\\\\ \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_2^2} & \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_3}\\\\ \\frac{\\partial^2 f(x)}{\\partial x_3 \\partial x_1} & \\frac{\\partial^2 f(x)}{\\partial x_3 \\partial x_2} & \\frac{\\partial^2 f(x)}{\\partial x_3^2}\\end{bmatrix}=\\begin{bmatrix}2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2\\end{bmatrix}$\n",
    "\n",
    "$0=H-\\lambda I=\\Bigg|\\begin{array}(2-\\lambda&0&0\\\\0&2-\\lambda&0\\\\0&0&2-\\lambda\\end{array}\\Bigg|$\n",
    "\n",
    "$\\lambda=2$\n",
    "\n",
    "Since the Hessian of this function is positive definite and the only constraints are that the solutions must be on a plane, which is a convex set, this problem is indeed convex.\n",
    "\n",
    "This problem can also be converted into an unconstrained optimization problem by simply solving the constraint equation for one of the variables and substituting into the minimized equation. I choose to solve for $x_1$ and sub it in...\n",
    "\n",
    "$x_1=1-2x_2-3x_3$\n",
    "\n",
    "$(x_1+1)^2+x_2^2+(x_3-1)^2=((1-2x_2-3x_3)+1)^2+x_2^2+(x_3-1)^2=(2-2x_2-3x_3)^2+x_2^2+(x_3-1)^2$ \n",
    "\n",
    "$$\\begin{aligned} &\\text{minimize:} &&5x_2^2+12x_2x_3-8x_2+10x_3^2-14x_3+5\\end{aligned}$$\n",
    "\n",
    "To confirm our previous finding that this problem is convex, we could find the eigenvalues of the Hessian of this new equation...\n",
    "\n",
    "$H(x)=\\begin{bmatrix}\\frac{\\partial^2 f(x)}{\\partial x_2^2} & \\frac{\\partial^2 f(x)}{\\partial x_2 \\partial x_3}\\\\ \\frac{\\partial^2 f(x)}{\\partial x_3 \\partial x_2} & \\frac{\\partial^2 f(x)}{\\partial x_3^2}\\end{bmatrix}=\\begin{bmatrix}10&12\\\\12&20\\end{bmatrix}$\n",
    "\n",
    "$0=H-\\lambda I=\\Bigg|\\begin{array}(10-\\lambda&12\\\\12&20-\\lambda\\end{array}\\Bigg|=(10-\\lambda)(20-\\lambda)-(12)^2=\\lambda^2-30\\lambda+56$\n",
    "\n",
    "$\\lambda=2,28$\n",
    "\n",
    "Confirming this using Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4354bcb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda= [ 2. 28.]\n"
     ]
    }
   ],
   "source": [
    "H=np.array([[10,12],[12,20]])\n",
    "eigs=np.linalg.eigvals(H)\n",
    "print('Lambda=',eigs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4daf0f",
   "metadata": {},
   "source": [
    "Thus confirms our prior conclusion that this is a convex problem.\n",
    "\n",
    "##### Solving the problem using gradient descent\n",
    "\n",
    "Now solving this new unconstrained problem using both the gradient descent method and Newton's method.\n",
    "# Code&Summary WIP\n",
    "\n",
    "##### Solving the problem using gradient descent\n",
    "\n",
    "# Code&Summary WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e796f",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "###### Proof that $af(x)+bg(x)$ is convex\n",
    "\n",
    "First, defining a new function...\n",
    "$h(x)=af(x)+bg(x)$\n",
    "\n",
    "Next, find the gradient and Hessian of this new function...\n",
    "\n",
    "$grad(x)=\\frac{\\partial h(x)}{\\partial x}=\\frac{d(af(x))}{dx}+\\frac{d(bg(x))}{dx}=af'(x)+bg'(x)$\n",
    "\n",
    "$H(x)=\\frac{\\partial grad(x)}{\\partial x}=\\frac{d(af'(x))}{dx}+\\frac{d(bg'(x))}{dx}=af''(x)+bg''(x)$\n",
    "\n",
    "As given, both f(x) and g(x) are convex thus their double derivatives are positive on the convex set $\\mathcal{X}$, this along with the fact that both a and b are $>0$ means that H(x) will always be positive, and thus positive definite and convex, on the convex set $\\mathcal{X}$.\n",
    "\n",
    "###### Finding the conditions under which f(g(x)) is convex\n",
    "\n",
    "Find the gradient and Hessian of this new function...\n",
    "\n",
    "$grad(x)=\\frac{\\partial f(g(x))}{\\partial x}=\\frac{df(g(x))}{dg(x)}\\frac{dg(x)}{dx}=f'(g(x))g'(x)$\n",
    "\n",
    "$H(x)=\\frac{\\partial f'(g(x))*g'(x)}{\\partial x}=\\frac{df'(g(x))}{\\partial x}\\frac{dg(x)}{d x}g'(x)+\\frac{dg'(x)}{dx}f'(g(x))$\n",
    "\n",
    "$H(x)=g'(x)^2f''(g(x))+g''(x)f'(g(x))$\n",
    "\n",
    "As given, both f(x) and g(x) are convex thus their double derivatives are positive on the convex set $\\mathcal{X}$, thus for $H(x)$ to be positive, making f(g(x)) convex, the sum of these two terms needs to be positive. This can happen in five cases:  \n",
    "1. $f''(g(x))\\ge0, f'(g(x))\\gt0$\n",
    "2. $g'(x)=0, f'(g(x))\\gt0$\n",
    "3. $g'(x)\\ne0, f''(g(x))\\gt0, f'(g(x))\\ge0$\n",
    "4. $g'(x)\\ne0, f''(g(x))\\gt0, f'(g(x))\\lt0, g'(x)^2f''(g(x))\\gt g''(x)f'(g(x))$\n",
    "5. $g'(x)\\ne0, f''(g(x))\\lt0, f'(g(x))\\gt0, g'(x)^2f''(g(x))\\lt g''(x)f'(g(x))$\n",
    "\n",
    "Note: It may be easy to confuse, but do not know if f''(g(x)) is positive on the convex set $\\mathcal{X}$, we only know that f(x) is convex on the convex set $\\mathcal{X}$, thus creating the complicated conditions for convexity listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a1c3f0",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "The Taylor series for a function, f(x) is as such...\n",
    "\n",
    "$f(x_1)=f(x_0)+g_{x0}^T(x_1-x_0)+\\frac12(x_1-x_0)^TH_{x0}(x_1-x_0)+O(||x_1-x_0||^2)$\n",
    "\n",
    "Since we know that f(x) is convex for $\\mathcal{X} \\to \\mathcal{R}$ we know that $H$ is positive semi-definite for all of $\\mathcal{X}$ and we know that any higher order trends will not influence the function to stop being convex, thus they must be either 0, or positive, in other words, posite semi-definite. Thus, we can say that the sum of these two p.s.d. terms is also p.s.d...\n",
    "\n",
    "$\\frac12(x_1-x_0)^TH_{x0}(x_1-x_0)+O(||x_1-x_0||^2)\\ge0$\n",
    "\n",
    "Neglecting these terms from the RHS of the original Taylor series then provides the claim...\n",
    "\n",
    "$f(x_1)\\ge f(x_0)+g_{x0}^T(x_1-x_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3de11",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "# WIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53669817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
